{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e71b34d-23a6-41a1-b657-c27ad1f9bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydromt_sfincs\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import datetime as dt\n",
    "import rioxarray as rio\n",
    "import numpy as np\n",
    "import hydromt\n",
    "from hydromt import DataCatalog\n",
    "from hydromt_sfincs import SfincsModel, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0b9e6fa-e627-4b5d-9502-95527671f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discrete_cmap(N, base_cmap=None):\n",
    "    \"\"\"Create an N-bin discrete colormap from the specified input map\"\"\"\n",
    "    base = plt.cm.get_cmap(base_cmap)\n",
    "    color_list = base(np.linspace(0, 1, N))\n",
    "    cmap_name = base.name + str(N)\n",
    "    out = LinearSegmentedColormap.from_list(cmap_name, color_list, N)\n",
    "    return out\n",
    "\n",
    "\n",
    "def hwm_to_gdf(csv_file_path, agency, quality=None, dst_crs=None):\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "\n",
    "    # If the HWM is downloaded from the USGS\n",
    "    if agency == 'usgs':\n",
    "        gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(x=df['longitude'], y=df['latitude'], crs=4326))\n",
    "        gdf['elev_m'] = gdf['elev_ft'] * 0.3048\n",
    "        if quality:\n",
    "            gdf = gdf[gdf['hwm_quality_id'] <= quality]\n",
    "        if dst_crs:\n",
    "            gdf.to_crs(dst_crs, inplace=True)\n",
    "        gdf = gdf[gdf['elev_m'].notna()]\n",
    "\n",
    "    # If the HWM data is from the NCEM\n",
    "    elif agency == 'ncem':\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(x=df['lon_dd'], y=df['lat_dd'], crs=4326))\n",
    "        gdf['elev_m'] = gdf['elev_ft'] * 0.3048\n",
    "        if dst_crs:\n",
    "            gdf.to_crs(dst_crs, inplace=True)\n",
    "        gdf = gdf[gdf['elev_m'].notna()]\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def calc_stats(observed, modeled):\n",
    "    mae = abs(observed - modeled).values.mean()\n",
    "    rmse = ((observed - modeled) ** 2).mean() ** 0.5\n",
    "    bias = (modeled - observed).values.mean()\n",
    "    return [round(mae, 2), round(rmse, 2), round(bias, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c2e039a-41ca-46f0-827b-4308872eff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in model and read results\n",
    "cat_dir = r'Z:\\users\\lelise\\data'\n",
    "yml_base_CONUS = os.path.join(cat_dir, 'data_catalog_BASE_CONUS.yml')\n",
    "yml_base_Carolinas = os.path.join(cat_dir, 'data_catalog_BASE_Carolinas.yml')\n",
    "yml_sfincs_Carolinas = os.path.join(cat_dir, 'data_catalog_SFINCS_Carolinas.yml')\n",
    "\n",
    "os.chdir(r'Z:\\users\\lelise\\projects\\Carolinas_SFINCS\\Chapter1_FlorenceValidation\\sfincs_models\\mod_v4_flor\\sensitivity_tests')\n",
    "model_root = 'ENC_200m_sbg5m_noChannels_avgN'\n",
    "\n",
    "mod = SfincsModel(root=model_root, mode='r',\n",
    "                  data_libs=[yml_base_CONUS, yml_base_Carolinas, yml_sfincs_Carolinas])\n",
    "cat = mod.data_catalog\n",
    "mod.read_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ac604f8-4890-4794-9d2b-f5d97f9108d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save data and figures to\n",
    "out_dir = os.path.join(mod.root, 'validation', 'hwm')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f7e652c-eb1d-4a9b-8ddb-be89071ac041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read USGS HWM data\n",
    "storm = 'florence'\n",
    "hwm_usgs = hwm_to_gdf(csv_file_path=rf'Z:\\users\\lelise\\data\\geospatial\\observations\\usgs_{storm}_FilteredHWMs.csv',\n",
    "                      agency='usgs',\n",
    "                      quality=3,\n",
    "                      dst_crs=mod.crs.to_epsg())\n",
    "hwm_usgs['data_source'] = 'USGS'\n",
    "hwm_usgs = hwm_usgs[hwm_usgs['stateName'].isin(['NC', 'SC'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81eee406-bdaa-49fd-b1b2-63d8a1a6b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read NCEM HWM file and write to CSV\n",
    "hwm_ncem = gpd.read_file(r'Z:\\users\\lelise\\data\\geospatial\\NC_State_Agencies\\NCEM_HWM\\HWM_master_share.gdb')\n",
    "hwm_ncem_df = pd.DataFrame(hwm_ncem)\n",
    "hwm_ncem_df.drop('geometry', inplace=True, axis=1)\n",
    "hwm_ncem_df[hwm_ncem_df.isnull()] = np.nan\n",
    "hwm_ncem_df[hwm_ncem_df.isna()] = np.nan\n",
    "hwm_ncem_df.to_csv(r'Z:\\users\\lelise\\data\\geospatial\\NC_State_Agencies\\NCEM_hwm_database_Sep2023.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11fc16cd-f2af-4d60-b77b-a83e9a3b53d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hurricane Florence' 'Hurricane Matthew' 'Asheville May 2018'\n",
      " 'Hurricane Floyd' 'Unknown' 'Hurricane Floyd?' 'Hurricane Fran'\n",
      " 'Hurricane Fran?' 'Unknown storm' 'Hurricane Helene' 'Hurricane Donna'\n",
      " 'Hurricane Connie' 'Hurricane Ione' 'Hurricane Diane' 'Hurricane Hazel'\n",
      " nan 'Heavy Rain' 'Unknown Storm' 'Hurricane Isabel' 'Hurricane Frances'\n",
      " 'Hurricane Ivan' 'Before Floyd' '1999 Storm' 'Tropical Storm 2006'\n",
      " 'Excessive Rain' 'Hurricane Irene' 'Hurricane Joaquin'\n",
      " 'Unspecified Hurricane' 'Hurricane Bonnie' 'Hurricane Diana'\n",
      " 'Hurricane Gloria' 'Hurricane Emily' 'Hurricane Bertha'\n",
      " 'Hurricane Ophelia' 'Hurricane Ernesto' 'Hurricane Dorian'\n",
      " 'Hurricane Isaias' 'TS Fred' 'TS Ophelia' 'Hurricane Idalia']\n",
      "['NCGS' 'nc_flood' 'USGS' 'USGS-USACE' 'USGS-HIST' 'USACE_HIST_FLOYD'\n",
      " 'USGS_HIST_FLOYD' 'DEWDAV_HIST_FRAN' 'USGS_HIST_FRAN' 'USACE_HIST_FRAN'\n",
      " 'nc_flood_p' 'USGS_HIST_BONNIE' 'USACE_HIST_BONNIE' 'USACE_HIST_NC'\n",
      " 'NOAA_HIST_NC' 'USGS_HIST_NC' 'ESPASSOC_HIST_NC' 'DEWDAV_HIST_NC'\n",
      " 'WSCONCPTS_HIST_NC' 'NCDOT']\n",
      "['NCGS' 'USGS']\n",
      "['Excellent: +/- 0.05 ft' 'Unknown/Historical' 'VP: > 0.40 ft'\n",
      " 'Poor: +/- 0.40 ft' 'Good: +/- 0.10 ft' 'Fair: +/- 0.20 ft']\n"
     ]
    }
   ],
   "source": [
    "# Read in NCEM HWM data\n",
    "hwm_ncem = hwm_to_gdf(csv_file_path=r'Z:\\users\\lelise\\data\\geospatial\\NC_State_Agencies\\NCEM_hwm_database_Sep2023.csv', agency='ncem',\n",
    "                      quality=None, dst_crs=mod.crs.to_epsg())\n",
    "print(hwm_ncem['storm_name'].unique())\n",
    "print(hwm_ncem['data_source'].unique())\n",
    "\n",
    "# Subset by storm of interest\n",
    "hwm_ncem_storm = hwm_ncem[hwm_ncem['storm_name'] == 'Hurricane Florence']\n",
    "print(hwm_ncem_storm['data_source'].unique())\n",
    "print(hwm_ncem_storm['confidence'].unique())\n",
    "\n",
    "# Remove data with Poor or lower quality\n",
    "quality_category = ['Unknown/Historical', 'VP: > 0.40 ft', 'Poor: +/- 0.40 ft']\n",
    "hwm_ncem_storm = hwm_ncem_storm.loc[~hwm_ncem_storm['confidence'].isin(quality_category)]\n",
    "hwm_ncem_storm = hwm_ncem_storm.loc[hwm_ncem_storm['data_source'] == 'NCGS']\n",
    "hwm_ncem_storm.columns = ['latitude_dd', 'longitude_dd', 'elev_ft', 'eventName',\n",
    "                          'data_source', 'hwmQualityName', 'geometry', 'elev_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b178717-1a2a-4fd9-83c8-a23fef02376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in gage peaks\n",
    "gage_stats = pd.read_csv(os.path.join(mod.root, 'validation', 'waterlevel', 'hydrograph_stats_by_gageID.csv'))\n",
    "gage_stats = gpd.GeoDataFrame(gage_stats,\n",
    "                              geometry=gpd.points_from_xy(x=gage_stats['x'], y=gage_stats['y'], crs=mod.crs))\n",
    "\n",
    "gage_stats = gage_stats[['pe', 'mod_peak_wl', 'obs_peak_wl', 'HUC6', 'source', 'geometry']]\n",
    "gage_stats.columns = ['error', 'sfincs_m', 'elev_m', 'Name', 'data_source', 'geometry']\n",
    "gage_stats['data_source'] = 'gage_' + gage_stats['data_source']\n",
    "gage_stats.drop(columns='Name', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9b46080-daad-4813-9709-5196669b2e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Peak WL datasets\n",
    "hwm = pd.concat([hwm_usgs, hwm_ncem_storm, gage_stats], axis=0, ignore_index=True)\n",
    "hwm = hwm.drop_duplicates(subset='geometry', keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c77f5225-4aba-40d2-a9a9-63cace2f1675",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Extract modeled water levels at HWMs and Calc Stats '''\n",
    "# Extract peak modeled water levels at the HWM points\n",
    "xcoords = hwm.geometry.x.to_xarray()\n",
    "ycoods = hwm.geometry.y.to_xarray()\n",
    "hwm['sfincs_m'] = mod.results['zsmax'].max(dim='timemax').sel(x=xcoords, y=ycoods,\n",
    "                                                              method='nearest').values.transpose()\n",
    "# Remove the locations outside the model domain\n",
    "hwm = hwm[hwm['sfincs_m'].notna()]\n",
    "hwm['error'] = hwm['sfincs_m'] - hwm['elev_m']\n",
    "mae, rmse, bias = calc_stats(observed=hwm['elev_m'], modeled=hwm['sfincs_m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4a55f02-c07e-43f1-995e-d84d0fa13087",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Calculate HWM stats by HUC6 Watershed '''\n",
    "# Assign to HUC6\n",
    "huc_boundary = gpd.read_file(r'Z:\\users\\lelise\\data\\geospatial\\hydrography\\nhd\\NHD_H_North_Carolina_State_Shape\\Shape'\n",
    "                             r'\\WBDHU6.shp')\n",
    "huc_boundary.to_crs(mod.crs, inplace=True)\n",
    "huc_boundary = huc_boundary[[\"HUC6\", \"Name\", \"geometry\"]]\n",
    "hwm = gpd.tools.sjoin(left_df=hwm, right_df=huc_boundary, how='left')\n",
    "hwm.drop(columns='index_right', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb1c40b0-ed50-4cf3-9316-3e088cbacb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign to State\n",
    "states = cat.get_geodataframe(\n",
    "    r'Z:\\users\\lelise\\data\\geospatial\\boundary\\us_boundary\\cb_2018_us_state_500k\\cb_2018_us_state_500k.shp')\n",
    "states = states[states['NAME'].isin(['South Carolina', 'North Carolina'])]\n",
    "states.to_crs(epsg=32617, inplace=True)\n",
    "states = states[['STUSPS', 'geometry']]\n",
    "hwm = gpd.tools.sjoin(left_df=hwm, right_df=states, how='left')\n",
    "hwm.drop(columns='index_right', inplace=True)\n",
    "hwm['xcoords'] = hwm.geometry.x.to_xarray()\n",
    "hwm['ycoords'] = hwm.geometry.y.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f1a3a2-9c15-41bb-a824-f79f099018df",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Extract Depth at locations '''\n",
    "sbg = cat.get_rasterdataset(os.path.join(mod.root, 'subgrid', 'dep_subgrid.tif'))\n",
    "hmax = utils.downscale_floodmap(\n",
    "    zsmax=mod.results[\"zsmax\"].max(dim='timemax'),\n",
    "    dep=sbg,\n",
    "    hmin=0.05,\n",
    "    gdf_mask=mod.region,\n",
    "    reproj_method='bilinear'\n",
    ")\n",
    "# hmax.raster.to_raster(os.path.join(os.getcwd(), 'floodmaps', f'noChannels.tif'), nodata=np.nan)\n",
    "\n",
    "xx = hwm['geometry'].x.to_xarray()\n",
    "yy = hwm['geometry'].y.to_xarray()\n",
    "hwm['sfincs_hmax_m'] = hmax.sel(x=xx, y=yy, method='nearest').values.transpose()\n",
    "hwm['sfincs_hmax_m'].fillna(0, inplace=True)\n",
    "hwm['height_above_gnd_m'] = hwm['height_above_gnd'] * 0.3048\n",
    "hwm['depth_error'] = hwm['sfincs_hmax_m'] - hwm['height_above_gnd_m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95930beb-d126-4cf5-bbd5-aafc82c281f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out all the HWM data\n",
    "hwm.to_csv(os.path.join(out_dir, 'hwm_error_all.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31e5fa26-c1c9-4625-93a1-e54287d88a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                mae  rmse  bias\n",
      "group                          \n",
      "SC             1.15  1.41  1.04\n",
      "NC             0.78  1.12  0.34\n",
      "Lower Pee Dee  0.93  1.21  0.71\n",
      "Neuse          0.65  0.87  0.26\n",
      "Cape Fear      1.15  1.54   0.6\n",
      "Pamlico        0.56  0.93  0.34\n",
      "Onslow Bay     0.44  0.58 -0.03\n",
      "domain         0.85  1.18  0.48\n"
     ]
    }
   ],
   "source": [
    "''' Calculate stats by grouping '''\n",
    "hwm['domain'] = 'domain'\n",
    "stats_by_group = pd.DataFrame()\n",
    "for group in ['STUSPS', 'Name', 'domain']:\n",
    "    for z in hwm[group].unique():\n",
    "        subset = hwm[hwm[group] == z]\n",
    "        ss = calc_stats(observed=subset['elev_m'], modeled=subset['sfincs_m'])\n",
    "        ss.append(z)\n",
    "        ss = pd.DataFrame(ss).T\n",
    "        ss.columns = ['mae', 'rmse', 'bias', 'group']\n",
    "        stats_by_group = pd.concat([stats_by_group, ss], axis=0, ignore_index=True)\n",
    "stats_by_group.dropna(axis=0, inplace=True)\n",
    "stats_by_group.set_index('group', inplace=True, drop=True)\n",
    "stats_by_group.to_csv(os.path.join(out_dir, 'peak_error_stats_by_group.csv'), index=True)\n",
    "print(stats_by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e932d68a-1601-4d59-8d05-9658caefb109",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'height_above_gnd_m'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\hydromt-sfincs-dev\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\hydromt-sfincs-dev\\lib\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\hydromt-sfincs-dev\\lib\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'height_above_gnd_m'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate depth stats\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(hwm[\u001b[38;5;241m~\u001b[39m\u001b[43mhwm\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheight_above_gnd_m\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misna()])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDepth stats calculated at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m locations\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m stats_by_group \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\hydromt-sfincs-dev\\lib\\site-packages\\geopandas\\geodataframe.py:1475\u001b[0m, in \u001b[0;36mGeoDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   1470\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;124;03m    If the result is a column containing only 'geometry', return a\u001b[39;00m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;124;03m    GeoSeries. If it's a DataFrame with any columns of GeometryDtype,\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m \u001b[38;5;124;03m    return a GeoDataFrame.\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1475\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1476\u001b[0m     \u001b[38;5;66;03m# Custom logic to avoid waiting for pandas GH51895\u001b[39;00m\n\u001b[0;32m   1477\u001b[0m     \u001b[38;5;66;03m# result is not geometry dtype for multi-indexes\u001b[39;00m\n\u001b[0;32m   1478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1479\u001b[0m         pd\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_scalar(key)\n\u001b[0;32m   1480\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1483\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_geometry_type(result)\n\u001b[0;32m   1484\u001b[0m     ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\hydromt-sfincs-dev\\lib\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\miniconda3\\envs\\hydromt-sfincs-dev\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'height_above_gnd_m'"
     ]
    }
   ],
   "source": [
    "# Calculate depth stats\n",
    "x = len(hwm[~hwm['height_above_gnd_m'].isna()])\n",
    "print(f'Depth stats calculated at {x} locations')\n",
    "stats_by_group = pd.DataFrame()\n",
    "for group in ['STUSPS', 'Name', 'domain']:\n",
    "    for z in hwm[group].unique():\n",
    "        subset = hwm[hwm[group] == z]\n",
    "        subset = subset[~subset['height_above_gnd_m'].isna()]\n",
    "        ss = calc_stats(observed=subset['height_above_gnd_m'], modeled=subset['sfincs_hmax_m'])\n",
    "        ss.append(z)\n",
    "        ss = pd.DataFrame(ss).T\n",
    "        ss.columns = ['mae', 'rmse', 'bias', 'group']\n",
    "        stats_by_group = pd.concat([stats_by_group, ss], axis=0, ignore_index=True)\n",
    "stats_by_group.dropna(axis=0, inplace=True)\n",
    "stats_by_group.set_index('group', inplace=True, drop=True)\n",
    "stats_by_group.to_csv(os.path.join(out_dir, 'hwm_stats_huc6_depth.csv'), index=True)\n",
    "print(stats_by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e8051c-c10b-4114-855c-2208973173c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
