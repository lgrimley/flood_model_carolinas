{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b04680e-56aa-44bb-acb3-79fb3e425f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import hydromt\n",
    "from hydromt import DataCatalog\n",
    "from hydromt_sfincs import SfincsModel, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce93f162-5ca1-487e-8932-413a4408adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "DESCRIPTION:\n",
    "    This script processes SFINCS model output to attribute peak water levels to the different flood processes or drivers.\n",
    "    Pieces of this code can be reused for a similar analysis.\n",
    "\n",
    "AUTHOR: Lauren Grimley\n",
    "CONTACT: lauren.grimley@unc.edu\n",
    "Updated: 8/6/24\n",
    "\n",
    "Input:\n",
    "    SFINCS model\n",
    "    Data array of downscaled peak flood depths\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b454ef90-ecff-4069-8750-6a05204f7a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_zsmax_by_driver(da, compound_key, runoff_key, coastal_key, name_out, hmin):\n",
    "    '''\n",
    "    Input\n",
    "        da = xr.datarray with the max water levels/flood depths for SFINCS runs where the index\n",
    "             is the model name to be queried using the compound, runoff, and coastal keys.\n",
    "     \n",
    "        hmin = minimum difference in water level between the compound and max individual\n",
    "    \n",
    "    Output:\n",
    "        da_classified = data array where the peak water level is attributed to a single flood processes using the hmin where\n",
    "                        (0 = none, 1 = coastal, 2 = coastal compounded, 3=runoff, 4=runoff compounded)\n",
    "\n",
    "        fld_area_by_driver = dataframe that is the total number of cells attributed to each flood process. \n",
    "                             This is used to calculate flood extent.\n",
    "\n",
    "        da_compound = data array mask of the compound peak flood extent\n",
    "\n",
    "        da_diff = data array of the compound scenario peak water levels minus the maximum individual\n",
    "    \n",
    "    '''\n",
    "    # Calculate the max water level at each cell across the coastal and runoff drivers\n",
    "    da_single_max = da.sel(run=[runoff_key, coastal_key]).max('run')\n",
    "    \n",
    "    # Calculate the difference between the max water level of the compound and the max of the individual drivers\n",
    "    da_diff = (da.sel(run=compound_key) - da_single_max).compute()\n",
    "    da_diff.name = 'diff in waterlevel compound minus max. single driver'\n",
    "    da_diff.attrs.update(unit='m')\n",
    "\n",
    "    # Create masks based on the driver that caused the max water level given a depth threshold hmin\n",
    "    compound_mask = da_diff > hmin\n",
    "    coastal_mask = da.sel(run=coastal_key).fillna(0) > da.sel(run=[runoff_key]).fillna(0).max('run')\n",
    "    runoff_mask = da.sel(run=runoff_key).fillna(0) > da.sel(run=[coastal_key]).fillna(0).max('run')\n",
    "    assert ~np.logical_and(runoff_mask, coastal_mask).any()\n",
    "    da_classified = (xr.where(coastal_mask, x=compound_mask + 1, y=0)\n",
    "                     + xr.where(runoff_mask, x=compound_mask + 3, y=0)).compute()\n",
    "    da_classified.name = name_out\n",
    "\n",
    "    # Calculate the number of cells that are attributed to the different drivers\n",
    "    unique_codes, fld_area_by_driver = np.unique(da_classified.data, return_counts=True)\n",
    "\n",
    "    # Return binary mask of compound peak flood extent\n",
    "    da_compound = xr.where(compound_mask, x=1, y=0)\n",
    "    da_compound.name = name_out\n",
    "\n",
    "    return da_classified, fld_area_by_driver, da_compound, da_diff\n",
    "\n",
    "\n",
    "def get_depth_stats(da_depth, var, thresholds):\n",
    "    df_depth = da_depth.to_dataframe()\n",
    "    df_depth.dropna(axis=0, inplace=True)\n",
    "    df_depth = pd.DataFrame(df_depth[var])\n",
    "\n",
    "    df_ss = pd.DataFrame()\n",
    "    for threshold in thresholds:\n",
    "        df_depth_sub = df_depth[df_depth[var] > threshold].astype(float).round(3)\n",
    "        depth_stats = df_depth_sub.describe(percentiles=[0.05, 0.5, 0.95])\n",
    "        depth_stats.columns = [f'depth_stats_{threshold}m']\n",
    "        df_ss = pd.concat([df_ss, depth_stats], axis=1, ignore_index=False)\n",
    "\n",
    "    return df_ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a7f68f5-7086-4aa9-a3ba-7ce61ed1a6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filepath to data catalog yml; read in model\n",
    "cat_dir = r'Z:\\users\\lelise\\data'\n",
    "yml_base_CONUS = os.path.join(cat_dir, 'data_catalog_BASE_CONUS.yml')\n",
    "yml_base_Carolinas = os.path.join(cat_dir, 'data_catalog_BASE_Carolinas.yml')\n",
    "yml_sfincs_Carolinas = os.path.join(cat_dir, 'data_catalog_SFINCS_Carolinas.yml')\n",
    "os.chdir(r'Z:\\users\\lelise\\projects\\Carolinas_SFINCS\\Chapter1_FlorenceValidation\\sfincs_models\\mod_v4_flor')\n",
    "model_root = r'ENC_200m_sbg5m_avgN_adv1_eff75'\n",
    "mod = SfincsModel(model_root, mode='r', data_libs=[yml_base_CONUS, yml_base_Carolinas, yml_sfincs_Carolinas])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aed7395b-871e-42e4-8719-d9ecca234491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ENC_200m_sbg5m_avgN_adv1_eff75' 'ENC_200m_sbg5m_avgN_adv1_eff75_coastal'\n",
      " 'ENC_200m_sbg5m_avgN_adv1_eff75_runoff'\n",
      " 'ENC_200m_sbg5m_avgN_adv1_eff75_discharge'\n",
      " 'ENC_200m_sbg5m_avgN_adv1_eff75_rainfall'\n",
      " 'ENC_200m_sbg5m_avgN_adv1_eff75_stormTide'\n",
      " 'ENC_200m_sbg5m_avgN_adv1_eff75_wind']\n"
     ]
    }
   ],
   "source": [
    "# Load peak flood maps and print run keys\n",
    "# Notes: the netcdf of peak water levels was created using the downscale_floodmaps.py script\n",
    "res = 200\n",
    "da = xr.open_dataarray(os.path.join(os.getcwd(), 'floodmaps',f'{res}m','floodmaps.nc'))\n",
    "print(da.run.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d30de85-1ad2-4992-9192-c82120e9a551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['compound' 'coastal' 'runoff' 'discharge' 'rainfall' 'stormTide' 'wind']\n"
     ]
    }
   ],
   "source": [
    "# Rename run IDs \n",
    "rename = ['compound','coastal','runoff','discharge','rainfall','stormTide','wind']\n",
    "da['run'] = xr.IndexVariable('run', rename) \n",
    "print(da.run.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6fedc0-4821-41b8-af37-d7bdae200fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in HUC6 watershed boundary\n",
    "huc_boundary = gpd.read_file(r'Z:\\users\\lelise\\data\\geospatial\\hydrography\\nhd\\NHD_H_North_Carolina_State_Shape\\Shape\\WBDHU6.shp')\n",
    "huc_boundary.to_crs(32617, inplace=True)\n",
    "huc_boundary = huc_boundary[[\"HUC6\", \"Name\", \"geometry\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bd4660b-9455-4bee-81d8-c5343fe18a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory to write files to\n",
    "out_dir = os.path.join(os.getcwd(), 'process_attribution',f'{res}m')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "os.chdir(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5813c3b2-cec0-4ef9-87c0-cb707a0142b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give the output filenames a unique ID\n",
    "outfile_ext = 'all'\n",
    "\n",
    "# # Comment the code out below if you want to get the full flood extent\n",
    "# # Create a raster mask of the large water bodies and gets overland flooding\n",
    "# coastal_wb = mod.data_catalog.get_geodataframe('carolinas_coastal_wb')\n",
    "# coastal_wb.to_crs(epsg=32617, inplace=True)\n",
    "# coastal_wb = coastal_wb.clip(mod.region)\n",
    "# coastal_wb[\"water\"] = 0\n",
    "# coastal_wb = da.raster.rasterize(coastal_wb, \"water\", nodata=1, all_touched=False)\n",
    "\n",
    "# # Mask the data that is located in large water bodies/estuaries\n",
    "# da = da.where(coastal_wb == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebd5686d-9044-49e3-ac11-e7a190518650",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Attribute total flood extent to runoff, coastal, and compound processes '''\n",
    "out = classify_zsmax_by_driver(da=da,\n",
    "                               compound_key='compound',\n",
    "                               runoff_key='runoff',\n",
    "                               coastal_key='coastal',\n",
    "                               name_out=out_dir,\n",
    "                               hmin=0.05\n",
    "                               )\n",
    "da_c, fld_cells_by_driver, da_compound, da_diff = out\n",
    "\n",
    "# Write to files\n",
    "da_c.to_netcdf(f'flor_peakWL_attributed_{outfile_ext}.nc')\n",
    "da_compound.to_netcdf(f'flor_peakWL_compound_extent_{outfile_ext}.nc')\n",
    "da_diff.to_netcdf(f'flor_peakWL_compound_minus_maxIndiv_{outfile_ext}.nc')\n",
    "da_diff.raster.to_raster(f'flor_peakWL_compound_minus_maxIndiv_{outfile_ext}.tif', nodata=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c32af9a2-0b90-4ca1-967f-984707133a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          0\n",
      "None              153190.96\n",
      "Coastal             7658.28\n",
      "Coastal Compound    3198.96\n",
      "Runoff             11333.92\n",
      "Runoff Compound      777.88\n"
     ]
    }
   ],
   "source": [
    "# Calculate flood area stats (sq.km)\n",
    "fld_area = fld_cells_by_driver.copy()\n",
    "res = da_diff.raster.res[0]  # meters\n",
    "fld_area = fld_area * (res * res) / (1000 ** 2)  # square km\n",
    "fld_area = pd.DataFrame(fld_area.T)\n",
    "fld_area.index = ['None', 'Coastal', 'Coastal Compound', 'Runoff', 'Runoff Compound']\n",
    "fld_area.to_csv(f'flor_peakWL_attributed_area_{outfile_ext}.csv')\n",
    "print(fld_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa846938-7dea-4432-b2cc-fc6a23c7b701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       depth_stats_-9999.0m  depth_stats_0.0m  depth_stats_0.05m\n",
      "count         573024.000000     470083.000000       99443.000000\n",
      "mean               0.021841          0.028908           0.098846\n",
      "std                0.046988          0.048451           0.065905\n",
      "min               -0.399000          0.000000           0.050000\n",
      "5%                -0.010000          0.000000           0.053000\n",
      "50%                0.005000          0.008000           0.081000\n",
      "95%                0.094000          0.102000           0.239000\n",
      "max                0.644000          0.644000           0.644000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the difference in water level stats\n",
    "var = 'diff in waterlevel compound minus max. single driver'\n",
    "depth_stats = get_depth_stats(da_depth=da_diff, var=var, thresholds=[-9999.0, 0.0, 0.05])\n",
    "depth_stats.to_csv(f'flor_peakWL_compound_minus_maxIndiv_stats_{outfile_ext}.csv')\n",
    "print(depth_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0892cc8a-fcbc-486e-bf6e-3e027e75e7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Pamlico      Neuse  Cape Fear  Onslow Bay  Lower Pee Dee\n",
      "None               10586.08   12249.92   20557.28     2055.68       23937.32\n",
      "Coastal             3970.20     581.24     137.20     1666.28         960.96\n",
      "Coastal Compound    1537.20    1224.48      86.52      280.44          70.12\n",
      "Runoff               474.12    1461.04    2979.48      429.92        5988.48\n",
      "Runoff Compound       94.52     208.44      93.88      237.60         143.44\n",
      "Compound          159497.88  160434.88  152305.64   171490.08      145059.68\n"
     ]
    }
   ],
   "source": [
    "''' Get flood area/depth information by HUC6 basin '''\n",
    "mdf = pd.DataFrame()\n",
    "for basin in ['Pamlico', 'Neuse', 'Cape Fear', 'Onslow Bay', 'Lower Pee Dee']:\n",
    "    sub = huc_boundary[huc_boundary['Name'] == basin]\n",
    "    sub[\"basin\"] = 1\n",
    "    b = da.raster.rasterize(sub, \"basin\", nodata=-9999.0, all_touched=False)\n",
    "    da2_c = da_c.where(b == 1)\n",
    "    da2_diff = da_diff.where(b==1)\n",
    "\n",
    "    unique_codes, fld_cells_by_driver = np.unique(da2_c.data, return_counts=True)\n",
    "    fld_area = fld_cells_by_driver.copy()\n",
    "\n",
    "    fld_area = fld_area * (res * res) / (1000 ** 2)  # square km\n",
    "    fld_area = pd.DataFrame(fld_area)\n",
    "    fld_area.index = ['None', 'Coastal', 'Coastal Compound', 'Runoff', 'Runoff Compound', 'Compound']\n",
    "    fld_area.columns = [basin]\n",
    "    mdf = pd.concat([mdf, fld_area], axis=1, ignore_index=False)\n",
    "\n",
    "mdf.to_csv(f'flor_peakWL_attributed_area_by_HUC.csv')\n",
    "print(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e62f1134-8d64-4b95-a2f9-d668f0741efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Part 3 - Attribute OVERLAND flood extent to flood drivers (e.g., forcings) '''\n",
    "\n",
    "def compute_waterlevel_difference(da, scen_base, scen_keys=None):\n",
    "    # Computer the difference in water level for compound compared to maximum single driver\n",
    "    da_single_max = da.sel(run=scen_keys).max('run')\n",
    "    da1 = (da.sel(run=scen_base) - da_single_max).compute()\n",
    "    da1.name = 'diff. in waterlevel\\ncompound - max. single driver'\n",
    "    da1.attrs.update(unit='m')\n",
    "    return da1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "991e04f0-d7f2-433b-a2cc-c5444799a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "da1 = compute_waterlevel_difference(da=da,\n",
    "                                    scen_base='coastal',\n",
    "                                    scen_keys=['stormTide','wind', 'discharge', 'rainfall']\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ec724f8-317d-4c79-ae8d-094e35d03ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = 0.05  # minimum difference in water level between compound scenario and max individual drivers\n",
    "compound_mask = da1 > dh  # mask of the cells where compound flooding occured\n",
    "# Now create masks for the individual drivers\n",
    "surge_mask = da.sel(run='stormTide').fillna(0) > da.sel(\n",
    "    run=['discharge', 'rainfall', 'wind']).fillna(0).max('run')\n",
    "coastal_mask = da.sel(run='wind').fillna(0) > da.sel(\n",
    "    run=['discharge', 'rainfall', 'stormTide']).fillna(0).max('run')\n",
    "discharge_mask = da.sel(run='discharge').fillna(0) > da.sel(\n",
    "    run=['wind', 'rainfall', 'stormTide']).fillna(0).max('run')\n",
    "precip_mask = da.sel(run='rainfall').fillna(0) > da.sel(\n",
    "    run=['wind', 'discharge', 'stormTide']).fillna(0).max('run')\n",
    "# precip_mask = np.logical_and(precip_mask, da1 >= 0)\n",
    "\n",
    "assert ~np.logical_and(precip_mask, surge_mask).any() and ~np.logical_and(precip_mask,\n",
    "                                                                          coastal_mask).any() and ~np.logical_and(\n",
    "    precip_mask, discharge_mask).any()\n",
    "\n",
    "assert ~np.logical_and(discharge_mask, surge_mask).any() and ~np.logical_and(discharge_mask,\n",
    "                                                                             coastal_mask).any()\n",
    "assert ~np.logical_and(surge_mask, coastal_mask).any()\n",
    "\n",
    "da_c = (\n",
    "        + xr.where(surge_mask, x=compound_mask + 1, y=0)\n",
    "        + xr.where(coastal_mask, x=compound_mask + 3, y=0)\n",
    "        + xr.where(discharge_mask, x=compound_mask + 5, y=0)\n",
    "        + xr.where(precip_mask, x=compound_mask + 7, y=0)\n",
    ").compute()\n",
    "da_c.name = None\n",
    "\n",
    "# Output the data array with driver attribution.\n",
    "# 1 = surge, 2 = surge compounded, 3 = wind, 4 = wind compounded\n",
    "# 5 = discharge, 6 = discharge compounded, 7 = rainfall, 8 = rainfall compounded\n",
    "da_c.to_netcdf(f'flor_peakWL_attributed_toDrivers_{outfile_ext}.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
